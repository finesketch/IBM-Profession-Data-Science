{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://www.bigdatauniversity.com\"><img src=\"https://ibm.box.com/shared/static/qo20b88v1hbjztubt06609ovs85q8fau.png\" width=\"400px\" align=\"center\"></a>\n",
    "\n",
    "<h1 align=\"center\"><font size=\"5\">TENSORFLOW'S HELLO WORLD</font></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n",
    "<font size = 3><strong>In this notebook we will overview the basics of TensorFlow, learn it's structure and see what is the motivation to use it</strong></font>\n",
    "<br>\n",
    "<h2>Table of Contents</h2>\n",
    "<ol>\n",
    "    <li><a href=\"#ref2\">How does TensorFlow work?</a></li>\n",
    "    <li><a href=\"#ref3\">Building a Graph</a></li>\n",
    "    <li><a href=\"#ref4\">Defining multidimensional arrays using TensorFlow</a></li>\n",
    "    <li><a href=\"#ref5\">Why Tensors?</a></li>\n",
    "    <li><a href=\"#ref6\">Variables</a></li>\n",
    "    <li><a href=\"#ref7\">Placeholders</a></li>\n",
    "    <li><a href=\"#ref8\">Operations</a></li>\n",
    "</ol>\n",
    "<p></p>\n",
    "</div>\n",
    "<br>\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"ref2\"></a>\n",
    "<h2>How does TensorFlow work?</h2>\n",
    "TensorFlow defines computations as Graphs, and these are made with operations (also know as “ops”). So, when we work with TensorFlow, it is the same as defining a series of operations in a Graph.\n",
    "\n",
    "To execute these operations as computations, we must launch the Graph into a Session. The session translates and passes the operations represented into the graphs to the device you want to execute them on, be it a GPU or CPU. In fact, TensorFlow's capability to execute the code on different devices such as CPUs and GPUs is a consequence of it's specific structure.\n",
    "\n",
    "For example, the image below represents a graph in TensorFlow. <b>W</b>, <b>x</b> and b are tensors over the edges of this graph. <b>MatMul</b> is an operation over the tensors <b>W</b> and <b>x</b>, after that <b>Add</b> is called and add the result of the previous operator with <b>b</b>. The resultant tensors of each operation cross the next one until the end where it's possible to get the wanted result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Collecting tensorflow\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e5/96/0c370ed7fc96e281aa4e93356cba8663d4295c8aad685d67ed1991aeaaff/tensorflow-2.3.0-cp37-cp37m-macosx_10_11_x86_64.whl (165.1MB)\n\u001b[K     |████████████████████████████████| 165.1MB 165kB/s \n\u001b[?25hCollecting h5py<2.11.0,>=2.10.0 (from tensorflow)\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1a/8b/4d01ae9a9d50a0bcc7b0b9aae41785d8d9de6fa9bba04dc20b1582181d2d/h5py-2.10.0-cp37-cp37m-macosx_10_6_intel.whl (3.0MB)\n\u001b[K     |████████████████████████████████| 3.0MB 12.6MB/s \n\u001b[?25hCollecting gast==0.3.3 (from tensorflow)\n  Downloading https://files.pythonhosted.org/packages/d6/84/759f5dd23fec8ba71952d97bcc7e2c9d7d63bdc582421f3cd4be845f0c98/gast-0.3.3-py2.py3-none-any.whl\nCollecting opt-einsum>=2.3.2 (from tensorflow)\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bc/19/404708a7e54ad2798907210462fd950c3442ea51acc8790f3da48d2bee8b/opt_einsum-3.3.0-py3-none-any.whl (65kB)\n\u001b[K     |████████████████████████████████| 71kB 12.4MB/s \n\u001b[?25hRequirement already satisfied: wheel>=0.26 in /Users/kyle/opt/anaconda3/lib/python3.7/site-packages (from tensorflow) (0.33.6)\nCollecting scipy==1.4.1 (from tensorflow)\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/85/7a/ae480be23b768910a9327c33517ced4623ba88dc035f9ce0206657c353a9/scipy-1.4.1-cp37-cp37m-macosx_10_6_intel.whl (28.4MB)\n\u001b[K     |████████████████████████████████| 28.4MB 981kB/s \n\u001b[?25hCollecting astunparse==1.6.3 (from tensorflow)\n  Downloading https://files.pythonhosted.org/packages/2b/03/13dde6512ad7b4557eb792fbcf0c653af6076b81e5941d36ec61f7ce6028/astunparse-1.6.3-py2.py3-none-any.whl\nRequirement already satisfied: six>=1.12.0 in /Users/kyle/opt/anaconda3/lib/python3.7/site-packages (from tensorflow) (1.12.0)\nCollecting grpcio>=1.8.6 (from tensorflow)\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/93/60/8a343ea6e2d8d4d7fb6701a80f27aef7ddadc8a2b26d151937f07c25f321/grpcio-1.30.0-cp37-cp37m-macosx_10_9_x86_64.whl (2.8MB)\n\u001b[K     |████████████████████████████████| 2.8MB 14.1MB/s \n\u001b[?25hCollecting absl-py>=0.7.0 (from tensorflow)\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1a/53/9243c600e047bd4c3df9e69cfabc1e8004a82cac2e0c484580a78a94ba2a/absl-py-0.9.0.tar.gz (104kB)\n\u001b[K     |████████████████████████████████| 112kB 13.4MB/s \n\u001b[?25hCollecting keras-preprocessing<1.2,>=1.1.1 (from tensorflow)\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/79/4c/7c3275a01e12ef9368a892926ab932b33bb13d55794881e3573482b378a7/Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42kB)\n\u001b[K     |████████████████████████████████| 51kB 10.0MB/s \n\u001b[?25hCollecting termcolor>=1.1.0 (from tensorflow)\n  Using cached https://files.pythonhosted.org/packages/8a/48/a76be51647d0eb9f10e2a4511bf3ffb8cc1e6b14e9e4fab46173aa79f981/termcolor-1.1.0.tar.gz\nRequirement already satisfied: wrapt>=1.11.1 in /Users/kyle/opt/anaconda3/lib/python3.7/site-packages (from tensorflow) (1.11.2)\nCollecting protobuf>=3.9.2 (from tensorflow)\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ed/81/af242fdec45e9046fba6b119748446310e62fc1100443e9fc7ff808c8228/protobuf-3.12.4-cp37-cp37m-macosx_10_9_x86_64.whl (1.3MB)\n\u001b[K     |████████████████████████████████| 1.3MB 14.7MB/s \n\u001b[?25hCollecting google-pasta>=0.1.8 (from tensorflow)\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/de/c648ef6835192e6e2cc03f40b19eeda4382c49b5bafb43d88b931c4c74ac/google_pasta-0.2.0-py3-none-any.whl (57kB)\n\u001b[K     |████████████████████████████████| 61kB 10.8MB/s \n\u001b[?25hRequirement already satisfied: numpy<1.19.0,>=1.16.0 in /Users/kyle/opt/anaconda3/lib/python3.7/site-packages (from tensorflow) (1.17.2)\nCollecting tensorflow-estimator<2.4.0,>=2.3.0 (from tensorflow)\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e9/ed/5853ec0ae380cba4588eab1524e18ece1583b65f7ae0e97321f5ff9dfd60/tensorflow_estimator-2.3.0-py2.py3-none-any.whl (459kB)\n\u001b[K     |████████████████████████████████| 460kB 18.4MB/s \n\u001b[?25hCollecting tensorboard<3,>=2.3.0 (from tensorflow)\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e9/1b/6a420d7e6ba431cf3d51b2a5bfa06a958c4141e3189385963dc7f6fbffb6/tensorboard-2.3.0-py3-none-any.whl (6.8MB)\n\u001b[K     |████████████████████████████████| 6.8MB 10.0MB/s \n\u001b[?25hRequirement already satisfied: setuptools in /Users/kyle/opt/anaconda3/lib/python3.7/site-packages (from protobuf>=3.9.2->tensorflow) (41.4.0)\nRequirement already satisfied: requests<3,>=2.21.0 in /Users/kyle/opt/anaconda3/lib/python3.7/site-packages (from tensorboard<3,>=2.3.0->tensorflow) (2.22.0)\nRequirement already satisfied: werkzeug>=0.11.15 in /Users/kyle/opt/anaconda3/lib/python3.7/site-packages (from tensorboard<3,>=2.3.0->tensorflow) (0.16.0)\nCollecting google-auth<2,>=1.6.3 (from tensorboard<3,>=2.3.0->tensorflow)\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cb/02/00e06ffa98fd0f11f36f808511012fa1fce41e4f79fa35dc7c515364ed01/google_auth-1.20.0-py2.py3-none-any.whl (91kB)\n\u001b[K     |████████████████████████████████| 92kB 15.8MB/s \n\u001b[?25hCollecting markdown>=2.6.8 (from tensorboard<3,>=2.3.0->tensorflow)\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a4/63/eaec2bd025ab48c754b55e8819af0f6a69e2b1e187611dd40cbbe101ee7f/Markdown-3.2.2-py3-none-any.whl (88kB)\n\u001b[K     |████████████████████████████████| 92kB 11.1MB/s \n\u001b[?25hCollecting tensorboard-plugin-wit>=1.6.0 (from tensorboard<3,>=2.3.0->tensorflow)\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b6/85/5c5ac0a8c5efdfab916e9c6bc18963f6a6996a8a1e19ec4ad8c9ac9c623c/tensorboard_plugin_wit-1.7.0-py3-none-any.whl (779kB)\n\u001b[K     |████████████████████████████████| 788kB 14.1MB/s \n\u001b[?25hCollecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard<3,>=2.3.0->tensorflow)\n  Downloading https://files.pythonhosted.org/packages/7b/b8/88def36e74bee9fce511c9519571f4e485e890093ab7442284f4ffaef60b/google_auth_oauthlib-0.4.1-py2.py3-none-any.whl\nRequirement already satisfied: certifi>=2017.4.17 in /Users/kyle/opt/anaconda3/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (2019.9.11)\nRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Users/kyle/opt/anaconda3/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (1.24.2)\nRequirement already satisfied: chardet<3.1.0,>=3.0.2 in /Users/kyle/opt/anaconda3/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (3.0.4)\nRequirement already satisfied: idna<2.9,>=2.5 in /Users/kyle/opt/anaconda3/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (2.8)\nCollecting cachetools<5.0,>=2.0.0 (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow)\n  Downloading https://files.pythonhosted.org/packages/cd/5c/f3aa86b6d5482f3051b433c7616668a9b96fbe49a622210e2c9781938a5c/cachetools-4.1.1-py3-none-any.whl\nCollecting rsa<5,>=3.1.4; python_version >= \"3.5\" (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow)\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1c/df/c3587a667d6b308fadc90b99e8bc8774788d033efcc70f4ecaae7fad144b/rsa-4.6-py3-none-any.whl (47kB)\n\u001b[K     |████████████████████████████████| 51kB 11.8MB/s \n\u001b[?25hCollecting pyasn1-modules>=0.2.1 (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow)\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/95/de/214830a981892a3e286c3794f41ae67a4495df1108c3da8a9f62159b9a9d/pyasn1_modules-0.2.8-py2.py3-none-any.whl (155kB)\n\u001b[K     |████████████████████████████████| 163kB 11.4MB/s \n\u001b[?25hRequirement already satisfied: importlib-metadata; python_version < \"3.8\" in /Users/kyle/opt/anaconda3/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow) (0.23)\nCollecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow)\n  Downloading https://files.pythonhosted.org/packages/a3/12/b92740d845ab62ea4edf04d2f4164d82532b5a0b03836d4d4e71c6f3d379/requests_oauthlib-1.3.0-py2.py3-none-any.whl\nCollecting pyasn1>=0.1.3 (from rsa<5,>=3.1.4; python_version >= \"3.5\"->google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow)\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/62/1e/a94a8d635fa3ce4cfc7f506003548d0a2447ae76fd5ca53932970fe3053f/pyasn1-0.4.8-py2.py3-none-any.whl (77kB)\n\u001b[K     |████████████████████████████████| 81kB 28.4MB/s \n\u001b[?25hRequirement already satisfied: zipp>=0.5 in /Users/kyle/opt/anaconda3/lib/python3.7/site-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow) (0.6.0)\nCollecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow)\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/05/57/ce2e7a8fa7c0afb54a0581b14a65b56e62b5759dbc98e80627142b8a3704/oauthlib-3.1.0-py2.py3-none-any.whl (147kB)\n\u001b[K     |████████████████████████████████| 153kB 19.2MB/s \n\u001b[?25hRequirement already satisfied: more-itertools in /Users/kyle/opt/anaconda3/lib/python3.7/site-packages (from zipp>=0.5->importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow) (7.2.0)\nBuilding wheels for collected packages: absl-py, termcolor\n  Building wheel for absl-py (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for absl-py: filename=absl_py-0.9.0-cp37-none-any.whl size=121932 sha256=377b2229c70d5161c971dcb41691fe69abf90e3cb8242521796ea2543bb87d68\n  Stored in directory: /Users/kyle/Library/Caches/pip/wheels/8e/28/49/fad4e7f0b9a1227708cbbee4487ac8558a7334849cb81c813d\n  Building wheel for termcolor (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for termcolor: filename=termcolor-1.1.0-cp37-none-any.whl size=4832 sha256=41003943810d34ce2e206c54579091b53db73907cda42491749210175d59855b\n  Stored in directory: /Users/kyle/Library/Caches/pip/wheels/7c/06/54/bc84598ba1daf8f970247f550b175aaaee85f68b4b0c5ab2c6\nSuccessfully built absl-py termcolor\nInstalling collected packages: h5py, gast, opt-einsum, scipy, astunparse, grpcio, absl-py, keras-preprocessing, termcolor, protobuf, google-pasta, tensorflow-estimator, cachetools, pyasn1, rsa, pyasn1-modules, google-auth, markdown, tensorboard-plugin-wit, oauthlib, requests-oauthlib, google-auth-oauthlib, tensorboard, tensorflow\n  Found existing installation: h5py 2.9.0\n    Uninstalling h5py-2.9.0:\n      Successfully uninstalled h5py-2.9.0\n  Found existing installation: scipy 1.3.1\n    Uninstalling scipy-1.3.1:\n      Successfully uninstalled scipy-1.3.1\nSuccessfully installed absl-py-0.9.0 astunparse-1.6.3 cachetools-4.1.1 gast-0.3.3 google-auth-1.20.0 google-auth-oauthlib-0.4.1 google-pasta-0.2.0 grpcio-1.30.0 h5py-2.10.0 keras-preprocessing-1.1.2 markdown-3.2.2 oauthlib-3.1.0 opt-einsum-3.3.0 protobuf-3.12.4 pyasn1-0.4.8 pyasn1-modules-0.2.8 requests-oauthlib-1.3.0 rsa-4.6 scipy-1.4.1 tensorboard-2.3.0 tensorboard-plugin-wit-1.7.0 tensorflow-2.3.0 tensorflow-estimator-2.3.0 termcolor-1.1.0\n"
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Collecting tensorflow-gpu\n\u001b[31m  ERROR: Could not find a version that satisfies the requirement tensorflow-gpu (from versions: none)\u001b[0m\n\u001b[31mERROR: No matching distribution found for tensorflow-gpu\u001b[0m\n"
    }
   ],
   "source": [
    "# error\n",
    "#!pip install --upgrade tensorflow-gpu\n",
    "\n",
    "!pip install --upgrade --force-reinstall tensorflow-gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://ibm.box.com/shared/static/a94cgezzwbkrq02jzfjjljrcaozu5s2q.png'>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Importing TensorFlow</h2>\n",
    "<p>To use TensorFlow, we need to import the library. We imported it and optionally gave it the name \"tf\", so the modules can be accessed by <b>tf.module-name</b>:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"ref3\"></a>\n",
    "# Building a Graph\n",
    "\n",
    "As we said before, TensorFlow works as a graph computational model. Let's create our first graph which we named as <b>graph1</b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph1 = tf.Graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we call the TensorFlow functions that construct new <b>tf.Operation</b> and <b>tf.Tensor</b> objects and add them to the <b>graph1</b>. As mentioned, each <b>tf.Operation</b> is a <b>node</b> and each <b>tf.Tensor</b> is an edge in the graph.\n",
    "\n",
    "Lets add 2 constants to our graph. For example, calling tf.constant([2], name = 'constant_a') adds a single <b>tf.Operation</b> to the default graph. This operation produces the value 2, and returns a <b>tf.Tensor</b> that represents the value of the constant.  \n",
    "<b>Notice:</b> tf.constant([2], name=\"constant_a\") creates a new tf.Operation named \"constant_a\" and returns a tf.Tensor named \"constant_a:0\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with graph1.as_default():\n",
    "    a = tf.constant([2], name = 'constant_a')\n",
    "    b = tf.constant([3], name = 'constant_b')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets look at the tensor __a__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tf.Tensor 'constant_a:0' shape=(1,) dtype=int32>"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, it just show the name, shape and type of the tensor in the graph. We will see it's value when we run it in a TensorFlow session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[2]\n"
    }
   ],
   "source": [
    "# Printing the value of a\n",
    "\n",
    "# tf.Session() depreciated\n",
    "\n",
    "sess = tf.compat.v1.Session(graph = graph1)\n",
    "result = sess.run(a)\n",
    "print(result)\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After that, let's make an operation over these tensors. The function <b>tf.add()</b> adds two tensors (you could also use `c = a + b`). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with graph1.as_default():\n",
    "    c = tf.add(a, b)\n",
    "    #c = a + b is also a way to define the sum of the terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then TensorFlow needs to initialize a session to run our code. Sessions are, in a way, a context for creating a graph inside TensorFlow. Let's define our session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.compat.v1.Session(graph = graph1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run the session to get the result from the previous defined 'c' operation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[5]\n"
    }
   ],
   "source": [
    "result = sess.run(c)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Close the session to release resources:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid having to close sessions every time, we can define them in a <b>with</b> block, so after running the <b>with</b> block the session will close automatically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[5]\n"
    }
   ],
   "source": [
    "with tf.compat.v1.Session(graph = graph1) as sess:\n",
    "    result = sess.run(c)\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even this silly example of adding 2 constants to reach a simple result defines the basis of TensorFlow. Define your operations (In this case our constants and _tf.add_), and start a session to build a graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>What is the meaning of Tensor?</h3>\n",
    "\n",
    "<div class=\"alert alert-success alertsuccess\" style=\"margin-top: 20px\">\n",
    "<font size = 3><strong>In TensorFlow all data is passed between operations in a computation graph, and these are passed in the form of Tensors, hence the name of TensorFlow.</strong></font>\n",
    "<br>\n",
    "<br>\n",
    "    The word <b>tensor</b> from new latin means \"that which stretches\". It is a mathematical object that is named \"tensor\" because an early application of tensors was the study of materials stretching under tension. The contemporary meaning of tensors can be taken as multidimensional arrays. \n",
    "\n",
    "\n",
    "\n",
    "</div>\n",
    "\n",
    "That's great, but... what are these multidimensional arrays? \n",
    "\n",
    "Going back a little bit to physics to understand the concept of dimensions:<br>\n",
    "<img src=\"https://ibm.box.com/shared/static/ymn0hl3hf8s3xb4k15v22y5vmuodnue1.svg\"/>\n",
    "<div style=\"text-align:center\"><a href=\"https://en.wikipedia.org/wiki/Dimension\">Image Source</a></div>\n",
    "<br>\n",
    "\n",
    "The zero dimension can be seen as a point, a single object or a single item.\n",
    "\n",
    "The first dimension can be seen as a line, a one-dimensional array can be seen as numbers along this line, or as points along the line. One dimension can contain infinite zero dimension/points elements.\n",
    "\n",
    "The second dimension can be seen as a surface, a two-dimensional array can be seen as an infinite series of lines along an infinite line. \n",
    "\n",
    "The third dimension can be seen as volume, a three-dimensional array can be seen as an infinite series of surfaces along an infinite line.\n",
    "\n",
    "The Fourth dimension can be seen as the hyperspace or spacetime, a volume varying through time, or an infinite series of volumes along an infinite line. And so forth on..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mathematical objects: <br><br>\n",
    "<img src=\"https://ibm.box.com/shared/static/kmxz570uai8eeg6i6ynqdz6kmlx1m422.png\">\n",
    "<div style=\"text-align: center\"><a href=\"https://book.mql4.com/variables/arrays\">Image Source</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summarizing:<br><br>\n",
    "<table style=\"width:100%\">\n",
    "  <tr>\n",
    "    <td><b>Dimension</b></td>\n",
    "    <td><b>Physical Representation</b></td> \n",
    "    <td><b>Mathematical Object</b></td>\n",
    "    <td><b>In Code</b></td>\n",
    "  </tr>\n",
    "  \n",
    "  <tr>\n",
    "    <td>Zero </td>\n",
    "    <td>Point</td> \n",
    "    <td>Scalar (Single Number)</td>\n",
    "    <td>[ 1 ]</td>\n",
    "  </tr>\n",
    "\n",
    "  <tr>\n",
    "    <td>One</td>\n",
    "    <td>Line</td> \n",
    "    <td>Vector (Series of Numbers) </td>\n",
    "    <td>[ 1,2,3,4,... ]</td>\n",
    "  </tr>\n",
    "  \n",
    "   <tr>\n",
    "    <td>Two</td>\n",
    "    <td>Surface</td> \n",
    "    <td>Matrix (Table of Numbers)</td>\n",
    "       <td>[ [1,2,3,4,...], [1,2,3,4,...], [1,2,3,4,...],... ]</td>\n",
    "  </tr>\n",
    "  \n",
    "   <tr>\n",
    "    <td>Three</td>\n",
    "    <td>Volume</td> \n",
    "    <td>Tensor (Cube of Numbers)</td>\n",
    "    <td>[ [[1,2,...], [1,2,...], [1,2,...],...], [[1,2,...], [1,2,...], [1,2,...],...], [[1,2,...], [1,2,...], [1,2,...] ,...]... ]</td>\n",
    "  </tr>\n",
    "  \n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"ref4\"></a>\n",
    "<h2>Defining multidimensional arrays using TensorFlow</h2>\n",
    "Now we will try to define such arrays using TensorFlow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Scalar (1 entry):\n 2 \n\nVector (3 entries) :\n [5 6 2] \n\nMatrix (3x3 entries):\n [[1 2 3]\n [2 3 4]\n [3 4 5]] \n\nTensor (3x3x3 entries) :\n [[[ 1  2  3]\n  [ 2  3  4]\n  [ 3  4  5]]\n\n [[ 4  5  6]\n  [ 5  6  7]\n  [ 6  7  8]]\n\n [[ 7  8  9]\n  [ 8  9 10]\n  [ 9 10 11]]] \n\n"
    }
   ],
   "source": [
    "graph2 = tf.Graph()\n",
    "with graph2.as_default():\n",
    "    Scalar = tf.constant(2)\n",
    "    Vector = tf.constant([5,6,2])\n",
    "    Matrix = tf.constant([[1,2,3],[2,3,4],[3,4,5]])\n",
    "    Tensor = tf.constant( [ [[1,2,3],[2,3,4],[3,4,5]] , [[4,5,6],[5,6,7],[6,7,8]] , [[7,8,9],[8,9,10],[9,10,11]] ] )\n",
    "with tf.compat.v1.Session(graph = graph2) as sess:\n",
    "    result = sess.run(Scalar)\n",
    "    print (\"Scalar (1 entry):\\n %s \\n\" % result)\n",
    "    result = sess.run(Vector)\n",
    "    print (\"Vector (3 entries) :\\n %s \\n\" % result)\n",
    "    result = sess.run(Matrix)\n",
    "    print (\"Matrix (3x3 entries):\\n %s \\n\" % result)\n",
    "    result = sess.run(Tensor)\n",
    "    print (\"Tensor (3x3x3 entries) :\\n %s \\n\" % result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>tf.shape</b> returns the shape of our data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "TensorShape([])"
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "source": [
    "Scalar.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "TensorShape([3, 3, 3])"
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "Tensor.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you understand these data structures, I encourage you to play with them using some previous functions to see how they will behave, according to their structure types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true,
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Defined using tensorflow function :\n[[3 4 5]\n [4 5 6]\n [5 6 7]]\nDefined using normal expressions :\n[[3 4 5]\n [4 5 6]\n [5 6 7]]\n"
    }
   ],
   "source": [
    "graph3 = tf.Graph()\n",
    "with graph3.as_default():\n",
    "    Matrix_one = tf.constant([[1,2,3],[2,3,4],[3,4,5]])\n",
    "    Matrix_two = tf.constant([[2,2,2],[2,2,2],[2,2,2]])\n",
    "\n",
    "    add_1_operation = tf.add(Matrix_one, Matrix_two)\n",
    "    add_2_operation = Matrix_one + Matrix_two\n",
    "\n",
    "with tf.compat.v1.Session(graph =graph3) as sess:\n",
    "    result = sess.run(add_1_operation)\n",
    "    print (\"Defined using tensorflow function :\")\n",
    "    print(result)\n",
    "    result = sess.run(add_2_operation)\n",
    "    print (\"Defined using normal expressions :\")\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the regular symbol definition and also the TensorFlow function we were able to get an element-wise multiplication, also known as Hadamard product. <br>\n",
    "\n",
    "But what if we want the regular matrix product?\n",
    "\n",
    "We then need to use another TensorFlow function called <b>tf.matmul()<b>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true,
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Defined using tensorflow function :\n[[13 18]\n [18 25]]\n"
    }
   ],
   "source": [
    "graph4 = tf.Graph()\n",
    "with graph4.as_default():\n",
    "    Matrix_one = tf.constant([[2,3],[3,4]])\n",
    "    Matrix_two = tf.constant([[2,3],[3,4]])\n",
    "\n",
    "    mul_operation = tf.matmul(Matrix_one, Matrix_two)\n",
    "\n",
    "with tf.compat.v1.Session(graph = graph4) as sess:\n",
    "    result = sess.run(mul_operation)\n",
    "    print (\"Defined using tensorflow function :\")\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could also define this multiplication ourselves, but there is a function that already does that, so no need to reinvent the wheel!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"ref5\"></a>\n",
    "<h2>Why Tensors?</h2>\n",
    "\n",
    "The Tensor structure helps us by giving the freedom to shape the dataset in the way we want.\n",
    "\n",
    "And it is particularly helpful when dealing with images, due to the nature of how information in images are encoded,\n",
    "\n",
    "Thinking about images, its easy to understand that it has a height and width, so it would make sense to represent the information contained in it with a two dimensional structure (a matrix)... until you remember that images have colors, and to add information about the colors, we need another dimension, and thats when Tensors become particularly helpful.\n",
    "\n",
    "Images are encoded into color channels, the image data is represented into each color intensity in a color channel at a given point, the most common one being RGB, which means Red, Blue and Green. The information contained into an image is the intensity of each channel color into the width and height of the image, just like this:\n",
    "\n",
    "<img src='https://ibm.box.com/shared/static/xlpv9h5xws248c09k1rlx7cer69y4grh.png'>\n",
    "<a href=\"https://msdn.microsoft.com/en-us/library/windows/desktop/dn424131.aspx\">Image Source</a>\n",
    "\n",
    "So the intensity of the red channel at each point with width and height can be represented into a matrix, the same goes for the blue and green channels, so we end up having three matrices, and when these are combined they form a tensor. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"ref6\"></a>\n",
    "# Variables\n",
    "\n",
    "Now that we are more familiar with the structure of data, we will take a look at how TensorFlow handles variables.\n",
    "<b>First of all, having tensors, why do we need variables?</b>  \n",
    "TensorFlow variables are used to share and persistent some stats that are manipulated by our program. That is, when you define a variable, TensorFlow adds a <b>tf.Operation</b> to your graph. Then, this operation will store a writable tensor value that persists between tf.Session.run calls. So, you can update the value of a variable through each run, while you cannot update tensor (e.g a tensor created by tf.constant()) through multiple runs in a session. \n",
    "\n",
    "<b>How to define a variable?</b>  \n",
    "To define variables we use the command <b>tf.Variable()</b>.\n",
    "To be able to use variables in a computation graph it is necessary to initialize them before running the graph in a session. This is done by running <b>tf.global_variables_initializer()</b>.\n",
    "\n",
    "To update the value of a variable, we simply run an assign operation that assigns a value to the variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "v = tf.Variable(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first create a simple counter, a variable that increases one unit at a time:\n",
    "\n",
    "To do this we use the <b>tf.assign(reference_variable, value_to_update)</b> command. <b>tf.assign</b> takes in two arguments, the <b>reference_variable</b> to update, and assign it to the <b>value_to_update</b> it by."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "update = tf.compat.v1.assign(v, v+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variables must be initialized by running an initialization operation after having launched the graph.  We first have to add the initialization operation to the graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init_op = tf.compat.v1.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then start a session to run the graph, first initialize the variables, then print the initial value of the <b>state</b> variable, and then run the operation of updating the <b>state</b> variable and printing the result after each update:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "The Session graph is empty.  Add operations to the graph before calling run().",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-97072d565214>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_op\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    956\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    957\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 958\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    959\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    960\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1104\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Attempted to use a closed Session.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1105\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1106\u001b[0;31m       raise RuntimeError('The Session graph is empty.  Add operations to the '\n\u001b[0m\u001b[1;32m   1107\u001b[0m                          'graph before calling run().')\n\u001b[1;32m   1108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The Session graph is empty.  Add operations to the graph before calling run()."
     ]
    }
   ],
   "source": [
    "with tf.compat.v1.Session() as session:\n",
    "    session.run(init_op)\n",
    "    print(session.run(v))\n",
    "    for _ in range(3):\n",
    "        session.run(update)\n",
    "        print(session.run(v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"ref7\"></a>\n",
    "# Placeholders\n",
    "\n",
    "Now we know how to manipulate variables inside TensorFlow graph, but what about feeding data outside of a TensorFlow graph? \n",
    "\n",
    "If you want to feed data to a TensorFlow graph from outside a graph, you will need to use placeholders.\n",
    "\n",
    "So <b>what are these placeholders and what do they do?</b> \n",
    "\n",
    "Placeholders can be seen as \"holes\" in your model, \"holes\" which you will pass the data to, you can create them using <br> <b>tf.placeholder(_datatype_)</b>, where <b>_datatype_</b> specifies the type of data (integers, floating points, strings, booleans) along with its precision (8, 16, 32, 64) bits.\n",
    "\n",
    "The definition of each data type with the respective python syntax is defined as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Data type\t|Python type|Description|\n",
    "| --------- | --------- | --------- |\n",
    "|DT_FLOAT\t|tf.float32\t|32 bits floating point.|\n",
    "|DT_DOUBLE\t|tf.float64\t|64 bits floating point.|\n",
    "|DT_INT8\t|tf.int8\t|8 bits signed integer.|\n",
    "|DT_INT16\t|tf.int16\t|16 bits signed integer.|\n",
    "|DT_INT32\t|tf.int32\t|32 bits signed integer.|\n",
    "|DT_INT64\t|tf.int64\t|64 bits signed integer.|\n",
    "|DT_UINT8\t|tf.uint8\t|8 bits unsigned integer.|\n",
    "|DT_STRING\t|tf.string\t|Variable length byte arrays. Each element of a Tensor is a byte array.|\n",
    "|DT_BOOL\t|tf.bool\t|Boolean.|\n",
    "|DT_COMPLEX64\t|tf.complex64\t|Complex number made of two 32 bits floating points: real and imaginary parts.|\n",
    "|DT_COMPLEX128\t|tf.complex128\t|Complex number made of two 64 bits floating points: real and imaginary parts.|\n",
    "|DT_QINT8\t|tf.qint8\t|8 bits signed integer used in quantized Ops.|\n",
    "|DT_QINT32\t|tf.qint32\t|32 bits signed integer used in quantized Ops.|\n",
    "|DT_QUINT8\t|tf.quint8\t|8 bits unsigned integer used in quantized Ops.|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we create a placeholder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "tf.placeholder() is not compatible with eager execution.",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-c566d7974bb1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36mplaceholder\u001b[0;34m(dtype, shape, name)\u001b[0m\n\u001b[1;32m   3095\u001b[0m   \"\"\"\n\u001b[1;32m   3096\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3097\u001b[0;31m     raise RuntimeError(\"tf.placeholder() is not compatible with \"\n\u001b[0m\u001b[1;32m   3098\u001b[0m                        \"eager execution.\")\n\u001b[1;32m   3099\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: tf.placeholder() is not compatible with eager execution."
     ]
    }
   ],
   "source": [
    "a = tf.compat.v1.placeholder(tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And define a simple multiplication operation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "An op outside of the function building code is being passed\na \"Graph\" tensor. It is possible to have Graph tensors\nleak out of the function building context by including a\ntf.init_scope in your function building code.\nFor example, the following function will fail:\n  @tf.function\n  def has_init_scope():\n    my_constant = tf.constant(1.)\n    with tf.init_scope():\n      added = my_constant * 2\nThe graph tensor has name: constant_a:0",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-913c5ff170cd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36mbinary_op_wrapper\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m   1139\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1140\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1141\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1142\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1143\u001b[0m           \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36mbinary_op_wrapper\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m   1123\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1124\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1125\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1126\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1127\u001b[0m         \u001b[0;31m# Even if dispatching the op failed, the RHS may be a tensor aware\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36m_mul_dispatch\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m   1455\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msparse_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSparseTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1456\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1457\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmultiply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1459\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36mmultiply\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m    507\u001b[0m   \"\"\"\n\u001b[1;32m    508\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 509\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    510\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36mmul\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m   6169\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6170\u001b[0m       return mul_eager_fallback(\n\u001b[0;32m-> 6171\u001b[0;31m           x, y, name=name, ctx=_ctx)\n\u001b[0m\u001b[1;32m   6172\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_SymbolicException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6173\u001b[0m       \u001b[0;32mpass\u001b[0m  \u001b[0;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36mmul_eager_fallback\u001b[0;34m(x, y, name, ctx)\u001b[0m\n\u001b[1;32m   6193\u001b[0m   \u001b[0m_attrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"T\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_attr_T\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6194\u001b[0m   _result = _execute.execute(b\"Mul\", 1, inputs=_inputs_flat, attrs=_attrs,\n\u001b[0;32m-> 6195\u001b[0;31m                              ctx=ctx, name=name)\n\u001b[0m\u001b[1;32m   6196\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0m_execute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmust_record_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6197\u001b[0m     _execute.record_gradient(\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     73\u001b[0m           \u001b[0;34m\"Inputs to eager execution function cannot be Keras symbolic \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m           \"tensors, but found {}\".format(keras_symbolic_tensors))\n\u001b[0;32m---> 75\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m   \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: An op outside of the function building code is being passed\na \"Graph\" tensor. It is possible to have Graph tensors\nleak out of the function building context by including a\ntf.init_scope in your function building code.\nFor example, the following function will fail:\n  @tf.function\n  def has_init_scope():\n    my_constant = tf.constant(1.)\n    with tf.init_scope():\n      added = my_constant * 2\nThe graph tensor has name: constant_a:0"
     ]
    }
   ],
   "source": [
    "b = a * 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to define and run the session, but since we created a \"hole\" in the model to pass the data, when we initialize the session we are obligated to pass an argument with the data, otherwise we would get an error.\n",
    "\n",
    "To pass the data into the model we call the session with an extra argument <b>feed_dict</b> in which we should pass a dictionary with each placeholder name followed by its respective data, just like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.compat.v1.Session() as sess:\n",
    "    result = sess.run(b,feed_dict={a:3.5})\n",
    "    print (result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since data in TensorFlow is passed in form of multidimensional arrays we can pass any kind of tensor through the placeholders to get the answer to the simple multiplication operation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dictionary={a: [ [ [1,2,3],[4,5,6],[7,8,9],[10,11,12] ] , [ [13,14,15],[16,17,18],[19,20,21],[22,23,24] ] ] }\n",
    "\n",
    "with tf.compat.v1.Session() as sess:\n",
    "    result = sess.run(b,feed_dict=dictionary)\n",
    "    print (result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"ref8\"></a>\n",
    "<hr>Operations</h2>\n",
    "\n",
    "Operations are nodes that represent the mathematical operations over the tensors on a graph. These operations can be any kind of functions, like add and subtract tensor or maybe an activation function.\n",
    "\n",
    "<b>tf.constant</b>, <b>tf.matmul</b>, <b>tf.add</b>, <b>tf.nn.sigmoid</b> are some of the operations in TensorFlow. These are like functions in python but operate directly over tensors and each one does a specific thing. \n",
    "<div class=\"alert alert-success alertsuccess\" style=\"margin-top: 20px\">Other operations can be easily found in: <a href=\"https://www.tensorflow.org/versions/r0.9/api_docs/python/index.html\">https://www.tensorflow.org/versions/r0.9/api_docs/python/index.html</a></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true,
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "c =: [7]\nd =: [3]\n"
    }
   ],
   "source": [
    "graph5 = tf.Graph()\n",
    "with graph5.as_default():\n",
    "    a = tf.constant([5])\n",
    "    b = tf.constant([2])\n",
    "    c = tf.add(a,b)\n",
    "    d = tf.subtract(a,b)\n",
    "\n",
    "with tf.compat.v1.Session(graph = graph5) as sess:\n",
    "    result = sess.run(c)\n",
    "    print ('c =: %s' % result)\n",
    "    result = sess.run(d)\n",
    "    print ('d =: %s' % result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>tf.nn.sigmoid</b> is an activation function, it's a little more complicated, but this function helps learning models to evaluate what kind of information is good or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Want to learn more?\n",
    "\n",
    "Running deep learning programs usually needs a high performance platform. __PowerAI__ speeds up deep learning and AI. Built on IBM’s Power Systems, __PowerAI__ is a scalable software platform that accelerates deep learning and AI with blazing performance for individual users or enterprises. The __PowerAI__ platform supports popular machine learning libraries and dependencies including TensorFlow, Caffe, Torch, and Theano. You can use [PowerAI on IMB Cloud](https://cocl.us/ML0120EN_PAI).\n",
    "\n",
    "Also, you can use __Watson Studio__ to run these notebooks faster with bigger datasets.__Watson Studio__ is IBM’s leading cloud solution for data scientists, built by data scientists. With Jupyter notebooks, RStudio, Apache Spark and popular libraries pre-packaged in the cloud, __Watson Studio__ enables data scientists to collaborate on their projects without having to install anything. Join the fast-growing community of __Watson Studio__ users today with a free account at [Watson Studio](https://cocl.us/ML0120EN_DSX).This is the end of this lesson. Thank you for reading this notebook, and good luck on your studies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thanks for completing this lesson!\n",
    "\n",
    "Notebook created by: <a href=\"https://linkedin.com/in/saeedaghabozorgi\"> Saeed Aghabozorgi </a> and <a href=\"https://ca.linkedin.com/in/rafaelblsilva\"> Rafael Belo Da Silva </a></h4> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.tensorflow.org/versions/r0.9/get_started/index.html<br>\n",
    "http://jrmeyer.github.io/tutorial/2016/02/01/TensorFlow-Tutorial.html<br>\n",
    "https://www.tensorflow.org/versions/r0.9/api_docs/python/index.html<br>\n",
    "<a href=\"https://www.tensorflow.org/api_docs/python/\">https://www.tensorflow.org/versions/r0.9/resources/dims_types.html</a><br>\n",
    "https://en.wikipedia.org/wiki/Dimension<br>\n",
    "https://book.mql4.com/variables/arrays<br>\n",
    "https://msdn.microsoft.com/en-us/library/windows/desktop/dn424131(v=vs.85).aspx<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "Copyright &copy; 2018 [Cognitive Class](https://cocl.us/DX0108EN_CC). This notebook and its source code are released under the terms of the [MIT License](https://bigdatauniversity.com/mit-license/)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}